---
layout: post
title: 【推荐系统】关于ALS的协同过滤
---





刚开始入推荐系统这个坑，发现要懂的东西有很多，ALS 主要用于**基于协同过滤的推荐系统**当中。
## 协同过滤算法

### 基本思想

协同过滤（Collaborative Filtering）推荐算法是最经典、最常用的推荐算法。基本思想是：

+ 根据用户之前的喜好以及其他兴趣相近的用户的选择来给用户推荐物品。

  + 基于对用户历史行为数据的挖掘发现用户的喜好偏向， 并预测用户可能喜好的产品进行推荐。
  + 一般是仅仅基于用户的行为数据（评价、购买、下载等）, 而不依赖于项的任何附加信息（物品自身特征）或者用户的任何附加信息（年龄， 性别等）。
+ 目前应用比较广泛的协同过滤算法是基于邻域的方法，主要有：
  + 基于用户的协同过滤算法（UserCF）：给用户推荐和他兴趣相似的其他用户喜欢的产品。
  + 基于物品的协同过滤算法（ItemCF）：给用户推荐和他之前喜欢的物品相似的物品。

不管是 UserCF 还是 ItemCF 算法， 重点是计算用户之间（或物品之间）的相似度。

### 相似性度量方法

1. **杰卡德（Jaccard）相似系数**
   
   `Jaccard` 系数是衡量两个集合的相似度一种指标，计算公式如下：
   $$
   sim_{uv}=\frac{|N(u) \cap N(v)|}{|N(u)| \cup|N(v)|}
   $$

   + 其中 $N(u)$，$N(v)$ 分别表示用户 $u$ 和用户 $v$ 交互物品的集合。
   
   + 对于用户 $u$ 和 $v$ ，该公式反映了两个交互物品交集的数量占这两个用户交互物品并集的数量的比例。
   
   由于杰卡德相似系数一般无法反映具体用户的评分喜好信息，所以常用来评估用户是否会对某物品进行打分， 而不是预估用户会对某物品打多少分。
   
2. **余弦相似度**
   余弦相似度衡量了两个向量的夹角，夹角越小越相似。余弦相似度的计算如下，其与杰卡德（Jaccard）相似系数只是在分母上存在差异：
   $$
   sim_{uv}=\frac{|N(u) \cap N(v)|}{\sqrt{|N(u)|\cdot|N(v)|}}
   $$
   从向量的角度进行描述，令矩阵 $A$ 为用户-物品交互矩阵，矩阵的行表示用户，列表示物品。
   
   + 设用户和物品数量分别为 $m,n$，交互矩阵$A$就是一个 $m$ 行 $n$ 列的矩阵。
   
   + 矩阵中的元素均为 $0/1$。若用户 $i$ 对物品 $j$ 存在交互，那么 $A_{i,j}=1$，否则为 $0$ 。

   + 那么，用户之间的相似度可以表示为：
     $$
     sim_{uv} = cos(u,v) =\frac{u\cdot v}{|u|\cdot |v|}
     $$
   
     + 向量 $u,v$ 在形式都是 one-hot 类型，$u\cdot v$ 表示向量点积。
   
   上述用户-物品交互矩阵在现实中是十分稀疏的，为了节省内存，交互矩阵会采用字典进行存储。在 `sklearn` 中，余弦相似度的实现：

   ```python
   from sklearn.metrics.pairwise import cosine_similarity
   
   i = [1, 0, 0, 0]
   j = [1, 0, 1, 0]
   cosine_similarity([i, j])
   ```
   
3. **皮尔逊相关系数**

   在用户之间的余弦相似度计算时，将用户向量的内积展开为各元素乘积和：
   $$
   sim_{uv} = \frac{\sum_i r_{ui}*r_{vi}}{\sqrt{\sum_i r_{ui}^2}\sqrt{\sum_i r_{vi}^2}}
   $$
   + 其中，$r_{ui},r_{vi}$ 分别表示用户 $u$ 和用户 $v$ 对物品 $i$ 是否有交互(或具体评分值)。
   
   皮尔逊相关系数与余弦相似度的计算公式非常的类似，如下：
   $$
   sim(u,v)=\frac{\sum_{i\in I}(r_{ui}-\bar r_u)(r_{vi}-\bar r_v)}{\sqrt{\sum_{i\in I }(r_{ui}-\bar r_u)^2}\sqrt{\sum_{i\in I }(r_{vi}-\bar r_v)^2}}
   $$
   + 其中，$r_{ui},r_{vi}$ 分别表示用户 $u$ 和用户 $v$ 对物品 $i$ 是否有交互(或具体评分值)；
   + $\bar r_u, \bar r_v$ 分别表示用户 $u$ 和用户 $v$ 交互的所有物品交互数量或者评分的平均值；
   
   相较于余弦相似度，皮尔逊相关系数通过使用用户的平均分对各独立评分进行修正，减小了用户评分偏置的影响。在`scipy`中，皮尔逊相关系数的实现：
   
   ```python
   from scipy.stats import pearsonr
   
   i = [1, 0, 0, 0]
   j = [1, 0.5, 0.5, 0]
   pearsonr(i, j)
   ```

**适用场景**

+ $Jaccard$ 相似度表示两个集合的交集元素个数在并集中所占的比例 ，所以适用于隐式反馈数据（0-1）。
+ 余弦相似度在度量文本相似度、用户相似度、物品相似度的时候都较为常用。
+ 皮尔逊相关度，实际上也是一种余弦相似度。不过先对向量做了中心化，范围在 $-1$ 到 $1$。
  + 相关度量的是两个变量的变化趋势是否一致，两个随机变量是不是同增同减。
  + 不适合用作计算布尔值向量（0-1）之间相关度。



### 算法评估

由于UserCF和ItemCF结果评估部分是共性知识点， 所以在这里统一标识。

#### 召回率

对用户 $u$ 推荐 $N$ 个物品记为 $R(u)$, 令用户 $u$ 在测试集上喜欢的物品集合为$T(u)$， 那么召回率定义为：
$$
\operatorname{Recall}=\frac{\sum_{u}|R(u) \cap T(u)|}{\sum_{u}|T(u)|}
$$

+ 含义：在模型召回预测的物品中，预测准确的物品占用户实际喜欢的物品的比例。 

#### 精确率

精确率定义为：
$$
\operatorname{Precision}=\frac{\sum_{u} \mid R(u) \cap T(u)|}{\sum_{u}|R(u)|}
$$

+ 含义：推荐的物品中，对用户准确推荐的物品占总物品的比例。 
+ 如要确保召回率高，一般是推荐更多的物品，期望推荐的物品中会涵盖用户喜爱的物品。而实际中，推荐的物品中用户实际喜爱的物品占少数，推荐的精确率就会很低。故同时要确保高召回率和精确率往往是矛盾的，所以实际中需要在二者之间进行权衡。

#### 覆盖率

覆盖率反映了推荐算法发掘长尾的能力， 覆盖率越高， 说明推荐算法越能将长尾中的物品推荐给用户。
$$
\text { Coverage }=\frac{\left|\bigcup_{u \in U} R(u)\right|}{|I|}
$$

+ 含义：推荐系统能够推荐出来的物品占总物品集合的比例。
  + 其中 $|I|$ 表示所有物品的个数；
  + 系统的用户集合为$U$;
  + 推荐系统给每个用户推荐一个长度为 $N$ 的物品列表$R(u)$.

+ 覆盖率表示最终的推荐列表中包含多大比例的物品。如果所有物品都被给推荐给至少一个用户， 那么覆盖率是100%。

#### 新颖度

用推荐列表中物品的平均流行度度量推荐结果的新颖度。 如果推荐出的物品都很热门， 说明推荐的新颖度较低。  由于物品的流行度分布呈长尾分布， 所以为了流行度的平均值更加稳定， 在计算平均流行度时对每个物品的流行度取对数。

- O’scar Celma 在博士论文 "[Music Recommendation and Discovery in the Long Tail](http://mtg.upf.edu/static/media/PhD_ocelma.pdf) " 中研究了新颖度的评测。







## 基于用户的协同过滤

### 基本思想

基于用户的协同过滤（UserCF）：

+ 例如，我们要对用户 $A$ 进行物品推荐，可以先找到和他有相似兴趣的其他用户。
+ 然后，将共同兴趣用户喜欢的，但用户 $A$ 未交互过的物品推荐给 $A$。  

<img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210629232540289.png" alt="image-20210629232540289" style="zoom: 80%;" />

### 计算过程

以下图为例，给用户推荐物品的过程可以形象化为一个猜测用户对物品进行打分的任务，表格里面是5个用户对于5件物品的一个打分情况，就可以理解为用户对物品的喜欢程度。

![image-20210629232622758](http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210629232622758.png)

UserCF算法的两个步骤：

+ 首先，根据前面的这些打分情况(或者说已有的用户向量）计算一下 Alice 和用户1， 2， 3， 4的相似程度， 找出与 Alice 最相似的 n 个用户。

+ 根据这 n 个用户对物品 5 的评分情况和与 Alice 的相似程度会猜测出 Alice 对物品5的评分。如果评分比较高的话， 就把物品5推荐给用户 Alice， 否则不推荐。

**具体过程：**

1. 计算用户之间的相似度

   + 根据 1.2 节的几种方法， 我们可以计算出各用户之间的相似程度。对于用户 Alice，选取出与其最相近的 $N$ 个用户。

2. 计算用户对新物品的评分预测

   + 常用的方式之一：利用目标用户与相似用户之间的相似度以及相似用户对物品的评分，来预测目标用户对候选物品的评分估计：
     $$
     R_{\mathrm{u}, \mathrm{p}}=\frac{\sum_{\mathrm{s} \in S}\left(w_{\mathrm{u}, \mathrm{s}} \cdot R_{\mathrm{s}, \mathrm{p}}\right)}{\sum_{\mathrm{s} \in S} w_{\mathrm{u}, \mathrm{s}}}
     $$

     + 其中，权重 $w_{u,s}$ 是用户 $u$ 和用户 $s$ 的相似度， $R_{s,p}$ 是用户 $s$ 对物品 $p$ 的评分。

   + 另一种方式：考虑到用户评分的偏置，即有的用户喜欢打高分， 有的用户喜欢打低分的情况。公式如下：
     $$
     R_{\mathrm{u}, \mathrm{p}}=\bar{R}_{u} + \frac{\sum_{\mathrm{s} \in S}\left(w_{\mathrm{u}, \mathrm{s}} \cdot \left(R_{s, p}-\bar{R}_{s}\right)\right)}{\sum_{\mathrm{s} \in S} w_{\mathrm{u}, \mathrm{s}}}
     $$

     + 其中，$\bar{R}_{s}$ 表示用户 $s$ 对物品的历史平均评分。

3. 对用户进行物品推荐

   + 在获得用户 $u$ 对不同物品的评价预测后， 最终的推荐列表根据预测评分进行排序得到。 

**手动计算：**

根据上面的问题， 下面手动计算 Alice 对物品 5 的得分：


1. 计算 Alice 与其他用户的相似度（基于皮尔逊相关系数）

   + 手动计算 Alice 与用户 1 之间的相似度：

   >用户向量 $\text {Alice}:(5,3,4,4)  , \text{user1}:(3,1,2,3) , \text {user2}:(  4,3,4,3) , \text {user3}:(3,3,1,5) , \text {user4}:(1,5,5,2) $
   >
   >+ 计算Alice与user1的余弦相似性:
   >$$
   >\operatorname{sim}(\text { Alice, user1 })=\cos (\text { Alice, user } 1)=\frac{15+3+8+12}{\operatorname{sqrt}(25+9+16+16) * \operatorname{sqrt}(9+1+4+9)}=0.975
   >$$
   >
   >+ 计算Alice与user1皮尔逊相关系数:
   >  + $Alice\_ave  =4 \quad  user1\_ave  =2.25 $
   >  + 向量减去均值: $\text {Alice}:(1,-1, 0,0) \quad \text { user1 }:(0.75,-1.25,-0.25,0.75)$
   >
   >+ 计算这俩新向量的余弦相似度和上面计算过程一致, 结果是 0.852 。
   >

   + 基于 sklearn 计算所有用户之间的皮尔逊相关系数。可以看出，与 Alice 相似度最高的用户为用户1和用户2。

     <img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/JavaWJkjP2fHH27Rajrj.png!thumbnail" alt="图片" style="zoom:80%;" />    

2. **根据相似度用户计算 Alice对物品5的最终得分**
    用户1对物品5的评分是3， 用户2对物品5的打分是5， 那么根据上面的计算公式， 可以计算出 Alice 对物品5的最终得分是
    $$
    P_{Alice, 物品5}=\bar{R}_{Alice}+\frac{\sum_{k=1}^{2}\left(w_{Alice,user k}\left(R_{userk, 物品5}-\bar{R}_{userk}\right)\right)}{\sum_{k=1}^{2} w_{Alice, userk}}=4+\frac{0.85*(3-2.4)+0.7*(5-3.8)}{0.85+0.7}=4.87
    $$
    
    + 同样方式，可以计算用户 Alice 对其他物品的评分预测。

3. **根据用户评分对用户进行推荐**
   
    + 根据 Alice 的打分对物品排个序从大到小：$$物品1>物品5>物品3=物品4>物品2$$。
    + 如果要向 Alice 推荐2款产品的话， 我们就可以推荐物品 1 和物品 5 给 Alice。

  至此， 基于用户的协同过滤算法原理介绍完毕。

### UserCF编程实现

1. 建立实验使用的数据表：

   ```python
   import numpy as np
   import pandas as pd
   
   
   def loadData():
       users = {'Alice': {'A': 5, 'B': 3, 'C': 4, 'D': 4},
                'user1': {'A': 3, 'B': 1, 'C': 2, 'D': 3, 'E': 3},
                'user2': {'A': 4, 'B': 3, 'C': 4, 'D': 3, 'E': 5},
                'user3': {'A': 3, 'B': 3, 'C': 1, 'D': 5, 'E': 4},
                'user4': {'A': 1, 'B': 5, 'C': 5, 'D': 2, 'E': 1}
                }
       return users
   ```

   + 这里使用字典来建立用户-物品的交互表。
     + 字典`users`的键表示不同用户的名字，值为一个评分字典，评分字典的键值对表示某物品被当前用户的评分。
     + 由于现实场景中，用户对物品的评分比较稀疏。如果直接使用矩阵进行存储，会存在大量空缺值，故此处使用了字典。

2. 计算用户相似性矩阵

   + 由于训练数据中共包含 5 个用户，所以这里的用户相似度矩阵的维度也为 $5 \times 5$。

   ```python
   user_data = loadData()
   similarity_matrix = pd.DataFrame(
       np.identity(len(user_data)),
       index=user_data.keys(),
       columns=user_data.keys(),
   )
   
   # 遍历每条用户-物品评分数据
   for u1, items1 in user_data.items():
       for u2, items2 in user_data.items():
           if u1 == u2:
               continue
           vec1, vec2 = [], []
           for item, rating1 in items1.items():
               rating2 = items2.get(item, -1)
               if rating2 == -1:
                   continue
               vec1.append(rating1)
               vec2.append(rating2)
           # 计算不同用户之间的皮尔逊相关系数
           similarity_matrix[u1][u2] = np.corrcoef(vec1, vec2)[0][1]
   
   print(similarity_matrix)
   ```

   ```
             1         2         3         4         5
   1  1.000000  0.852803  0.707107  0.000000 -0.792118
   2  0.852803  1.000000  0.467707  0.489956 -0.900149
   3  0.707107  0.467707  1.000000 -0.161165 -0.466569
   4  0.000000  0.489956 -0.161165  1.000000 -0.641503
   5 -0.792118 -0.900149 -0.466569 -0.641503  1.000000
   ```

3. 计算与 Alice 最相似的 `num` 个用户

   ```python
   target_user = ' Alice '
   num = 2
   # 由于最相似的用户为自己，去除本身
   sim_users = similarity_matrix[target_user].sort_values(ascending=False)[1:num+1].index.tolist()
   print(f'与用户{target_user}最相似的{num}个用户为：{sim_users}')
   ```

   ```
   与用户 Alice 最相似的2个用户为：['user1', 'user2']
   ```

4. 预测用户 Alice 对物品 `E` 的评分

   ```python
   weighted_scores = 0.
   corr_values_sum = 0.
   
   target_item = 'E'
   # 基于皮尔逊相关系数预测用户评分
   for user in sim_users:
       corr_value = similarity_matrix[target_user][user]
       user_mean_rating = np.mean(list(user_data[user].values()))
   
       weighted_scores += corr_value * (user_data[user][target_item] - user_mean_rating)
       corr_values_sum += corr_value
   
   target_user_mean_rating = np.mean(list(user_data[target_user].values()))
   target_item_pred = target_user_mean_rating + weighted_scores / corr_values_sum
   print(f'用户{target_user}对物品{target_item}的预测评分为：{target_item_pred}')
   ```

   ```
   用户 Alice 对物品E的预测评分为：4.871979899370592
   ```

### UserCF优缺点

User-based算法存在两个重大问题：


1. 数据稀疏性
  + 一个大型的电子商务推荐系统一般有非常多的物品，用户可能买的其中不到1%的物品，不同用户之间买的物品重叠性较低，导致算法无法找到一个用户的邻居，即偏好相似的用户。
  + 这导致UserCF不适用于那些正反馈获取较困难的应用场景(如酒店预订， 大件物品购买等低频应用)。

1. 算法扩展性
  + 基于用户的协同过滤需要维护用户相似度矩阵以便快速的找出 $TopN$ 相似用户， 该矩阵的存储开销非常大，存储空间随着用户数量的增加而增加。
  + 故不适合用户数据量大的情况使用。

由于UserCF技术上的两点缺陷， 导致很多电商平台并没有采用这种算法， 而是采用了ItemCF算法实现最初的推荐系统。





### 参考资料

* [基于用户的协同过滤来构建推荐系统：https://mp.weixin.qq.com/s/ZtnaQrVIpVOPJpqMdLWOcw](https://mp.weixin.qq.com/s/ZtnaQrVIpVOPJpqMdLWOcw)
* [协同过滤算法概述：https://chenk.tech/posts/8ad63d9d.html](https://chenk.tech/posts/8ad63d9d.html)
* B站黑马推荐系统实战课程











## 隐语义模型与矩阵分解

协同过滤算法的特点：

+ 协同过滤算法的特点就是完全没有利用到物品本身或者是用户自身的属性， 仅仅利用了用户与物品的交互信息就可以实现推荐，是一个可解释性很强， 非常直观的模型。
+ 但是也存在一些问题，处理稀疏矩阵的能力比较弱。

为了使得协同过滤更好处理稀疏矩阵问题， 增强泛化能力。从协同过滤中衍生出矩阵分解模型(Matrix Factorization, MF)或者叫隐语义模型：

+ 在协同过滤共现矩阵的基础上， 使用更稠密的隐向量表示用户和物品。
+ 通过挖掘用户和物品的隐含兴趣和隐含特征， 在一定程度上弥补协同过滤模型处理稀疏矩阵能力不足的问题。



### 隐语义模型

隐语义模型最早在文本领域被提出，用于找到文本的隐含语义。在2006年， 被用于推荐中， 它的核心思想是通过隐含特征（latent  factor）联系用户兴趣和物品（item）， 基于用户的行为找出潜在的主题和分类， 然后对物品进行自动聚类，划分到不同类别/主题(用户的兴趣)。

以项亮老师《推荐系统实践》书中的内容为例：

>如果我们知道了用户A和用户B两个用户在豆瓣的读书列表， 从他们的阅读列表可以看出，用户A的兴趣涉及侦探小说、科普图书以及一些计算机技术书， 而用户B的兴趣比较集中在数学和机器学习方面。 那么如何给A和B推荐图书呢？ 先说说协同过滤算法， 这样好对比不同：
>* 对于UserCF，首先需要找到和他们看了同样书的其他用户（兴趣相似的用户），然后给他们推荐那些用户喜欢的其他书。 
>* 对于ItemCF，需要给他们推荐和他们已经看的书相似的书，比如作者B看了很多关于数据挖掘的书，可以给他推荐机器学习或者模式识别方面的书。 
>
>而如果是隐语义模型的话，  它会先通过一些角度把用户兴趣和这些书归一下类， 当来了用户之后， 首先得到他的兴趣分类， 然后从这个分类中挑选他可能喜欢的书籍。 

隐语义模型和协同过滤的不同主要体现在隐含特征上， 比如书籍的话它的内容， 作者， 年份， 主题等都可以算隐含特征。

以王喆老师《深度学习推荐系统》中的一个原理图为例，看看是如何通过隐含特征来划分开用户兴趣和物品的。

<img src="https://img-blog.csdnimg.cn/20200822212051499.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" style="zoom:80%;" />

#### 音乐评分实例

假设每个用户都有自己的听歌偏好， 比如用户 A 喜欢带有**小清新的**， **吉他伴奏的**， **王菲**的歌曲，如果一首歌正好**是王菲唱的， 并且是吉他伴奏的小清新**， 那么就可以将这首歌推荐给这个用户。 也就是说是**小清新， 吉他伴奏， 王菲**这些元素连接起了用户和歌曲。 

当然每个用户对不同的元素偏好不同， 每首歌包含的元素也不一样， 所以我们就希望找到下面的两个矩阵：

1. 潜在因子—— 用户矩阵Q
    这个矩阵表示不同用户对于不同元素的偏好程度， 1代表很喜欢， 0代表不喜欢， 比如下面这样：

  <div align=center>
  <img src="https://img-blog.csdnimg.cn/2020082222025968.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" style="zoom:70%;" />
  </div>
2. 潜在因子——音乐矩阵P
    表示每种音乐含有各种元素的成分， 比如下表中， 音乐A是一个偏小清新的音乐， 含有小清新的Latent Factor的成分是0.9， 重口味的成分是0.1， 优雅成分0.2...

  <div align=center>
  <img src="https://img-blog.csdnimg.cn/20200822220751394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" style="zoom:70%;" />
  </div>
**计算张三对音乐A的喜爱程度**

利用上面的这两个矩阵，将对应向量进行内积计算，我们就能得出张三对音乐A的喜欢程度：

<div align=center>
<img src="https://img-blog.csdnimg.cn/20200822221627219.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" style="zoom:55%;" />
</div>

+ 张三对**小清新**的偏好 * 音乐A含有**小清新**的成分 + 张三对**重口味**的偏好 * 音乐A含有**重口味**的成分 + 张三对**优雅**的偏好 * 音乐A含有**优雅**的成分... 

+ 根据隐向量其实就可以得到张三对音乐A的打分，即： $$0.6 * 0.9 + 0.8 * 0.1 + 0.1 * 0.2 + 0.1 * 0.4 + 0.7 * 0 = 0.68$$。

**计算所有用户对不同音乐的喜爱程度**

按照这个计算方式， 每个用户对每首歌其实都可以得到这样的分数， 最后就得到了我们的评分矩阵：

<div align=center>
<img src="https://img-blog.csdnimg.cn/20200822222141231.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" style="zoom:60%;" />
</div>
+ 红色部分表示用户没有打分，可以通过隐向量计算得到的。 

**小结**

+ 上面例子中的小清晰， 重口味， 优雅这些就可以看做是隐含特征， 而通过这个隐含特征就可以把用户的兴趣和音乐的进行一个分类， 其实就是找到了每个用户每个音乐的一个隐向量表达形式（与深度学习中的embedding等价）
+ 这个隐向量就可以反映出用户的兴趣和物品的风格，并能将相似的物品推荐给相似的用户等。 **有没有感觉到是把协同过滤算法进行了一种延伸， 把用户的相似性和物品的相似性通过了一个叫做隐向量的方式进行表达**

+ 现实中，类似于上述的矩阵 $P,Q$ 一般很难获得。有的只是用户的评分矩阵，如下：

  <div align=center>
  <img src="https://img-blog.csdnimg.cn/20200822223313349.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" style="zoom:60%;" />
  </div>

  + 这种矩阵非常的稀疏，如果直接基于用户相似性或者物品相似性去填充这个矩阵是不太容易的。
  + 并且很容易出现长尾问题， 而矩阵分解就可以比较容易的解决这个问题。

+ 矩阵分解模型:

  + 基于评分矩阵，将其分解成Q和P两个矩阵乘积的形式，获取用户兴趣和物品的隐向量表达。
  + 然后，基于两个分解矩阵去预测某个用户对某个物品的评分了。 
  + 最后，基于预测评分去进行物品推荐。

  

### 矩阵分解算法ALS

#### ALS算法概括

1. ALS算法用来补全用户评分矩阵。由于用户评分矩阵比较稀疏，将用户评分矩阵进行分解，变成V和U的乘积。通过求得V和U两个小的矩阵来补全用户评分矩阵。
2. ALS算法使用**交替最小二乘法**来进行求解
3. ALS分为显示反馈和隐式反馈两种。显示反馈是指用户有明确的评分。对于商品推荐来说，大部分是通过用户的行为，获取隐式反馈的评分。隐式反馈评分矩阵需要进行处理，如果有用户评分则置为1，没有则赋值为0。但是对这个处理后的评分矩阵，再有一个置信度来评价这个评分。置信度等于1+a*用户真实评分
4. ALS的代价函数是估计值和现有的评分值误差的平方和，引入了L2正则

<div align=center>
<img src="https://img-blog.csdnimg.cn/20201022091004550.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" style="zoom:70%;" />
</div>


#### 算法原理

在矩阵分解的算法框架下， **可以通过分解协同过滤的共现矩阵（评分矩阵）来得到用户和物品的隐向量**，原理如下：。

<div align=center>
<img src="https://img-blog.csdnimg.cn/20200823101513233.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3d1emhvbmdxaWFuZw==,size_1,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" style="zoom:70%;" />
</div>
+ 矩阵分解算法将 $m\times n$ 维的共享矩阵 $R$ ，分解成 $m \times k$ 维的用户矩阵 $U$ 和 $k \times n$ 维的物品矩阵 $V$ 相乘的形式。
+ 其中，$m$ 是用户数量， $n$ 是物品数量， $k$ 是隐向量维度， 也就是隐含特征个数。
+ 这里的隐含特征没有太好的可解释性，需要模型自己去学习。
+ 一般而言， $k$ 越大隐向量能承载的信息内容越多，表达能力也会更强，但相应的学习难度也会增加。所以，我们需要根据训练集样本的数量去选择合适的数值，在保证信息学习相对完整的前提下，降低模型的学习难度。





### 编程实现

本小节，使用如下图表来预测Alice对物品5的评分：

<div align=center>
<img src="https://img-blog.csdnimg.cn/20200827150237921.png#pic_center" alt="在这里插入图片描述" style="zoom:80%;" />
</div>
基于矩阵分解算法的流程如下：

1.  首先， 它会先初始化用户矩阵 $P$ 和物品矩阵 $Q$ ， $P$ 的维度是`[users_num, K]`，$Q$ 的维度是`[items_num, K]`， 

   + 其中，`F`表示隐向量的维度。 也就是把通过隐向量的方式把用户的兴趣和`F`的特点关联了起来。  

   + 初始化这两个矩阵的方式很多， 但根据经验， 随机数需要和`1/sqrt(F)`成正比。 

2.  根据预测评分和真实评分的偏差，利用梯度下降法进行参数更新。

   + 遍历用户及其交互过的物品，对已交互过的物品进行评分预测。
   + 由于预测评分与真实评分存在偏差， 再根据第3节的梯度更新公式更新参数。

3. 训练完成后，利用用户向量与目标物品向量的内积进行评分预测。

**完整代码如下：**

```python
import random
import math


class BiasSVD():
    def __init__(self, rating_data, F=5, alpha=0.1, lmbda=0.1, max_iter=100):
        self.F = F          # 这个表示隐向量的维度
        self.P = dict()     # 用户矩阵P  大小是[users_num, F]
        self.Q = dict()     # 物品矩阵Q  大小是[item_nums, F]
        self.bu = dict()    # 用户偏置系数
        self.bi = dict()    # 物品偏置系数
        self.mu = 0         # 全局偏置系数
        self.alpha = alpha  # 学习率
        self.lmbda = lmbda  # 正则项系数
        self.max_iter = max_iter        # 最大迭代次数
        self.rating_data = rating_data  # 评分矩阵

        for user, items in self.rating_data.items():
            # 初始化矩阵P和Q, 随机数需要和1/sqrt(F)成正比
            self.P[user] = [random.random() / math.sqrt(self.F) for x in range(0, F)]
            self.bu[user] = 0
            for item, rating in items.items():
                if item not in self.Q:
                    self.Q[item] = [random.random() / math.sqrt(self.F) for x in range(0, F)]
                    self.bi[item] = 0

    # 采用随机梯度下降的方式训练模型参数
    def train(self):
        cnt, mu_sum = 0, 0
        for user, items in self.rating_data.items():
            for item, rui in items.items():
                mu_sum, cnt = mu_sum + rui, cnt + 1
        self.mu = mu_sum / cnt

        for step in range(self.max_iter):
            # 遍历所有的用户及历史交互物品
            for user, items in self.rating_data.items():
                # 遍历历史交互物品
                for item, rui in items.items():
                    rhat_ui = self.predict(user, item)  # 评分预测
                    e_ui = rui - rhat_ui  				# 评分预测偏差

                    # 参数更新
                    self.bu[user] += self.alpha * (e_ui - self.lmbda * self.bu[user])
                    self.bi[item] += self.alpha * (e_ui - self.lmbda * self.bi[item])
                    for k in range(0, self.F):
                        self.P[user][k] += self.alpha * (e_ui * self.Q[item][k] - self.lmbda * self.P[user][k])
                        self.Q[item][k] += self.alpha * (e_ui * self.P[user][k] - self.lmbda * self.Q[item][k])
            # 逐步降低学习率
            self.alpha *= 0.1


    # 评分预测
    def predict(self, user, item):
        return sum(self.P[user][f] * self.Q[item][f] for f in range(0, self.F)) + self.bu[user] + self.bi[
            item] + self.mu


# 通过字典初始化训练样本，分别表示不同用户（1-5）对不同物品（A-E)的真实评分
def loadData():
    rating_data={1: {'A': 5, 'B': 3, 'C': 4, 'D': 4},
           2: {'A': 3, 'B': 1, 'C': 2, 'D': 3, 'E': 3},
           3: {'A': 4, 'B': 3, 'C': 4, 'D': 3, 'E': 5},
           4: {'A': 3, 'B': 3, 'C': 1, 'D': 5, 'E': 4},
           5: {'A': 1, 'B': 5, 'C': 5, 'D': 2, 'E': 1}
          }
    return rating_data

# 加载数据
rating_data = loadData()
# 建立模型
basicsvd = BiasSVD(rating_data, F=10)
# 参数训练
basicsvd.train()
# 预测用户1对物品E的评分
for item in ['E']:
    print(item, basicsvd.predict(1, item))

# 预测结果：E 3.685084274454321
```
### 课后思考

1. 矩阵分解算法后续有哪些改进呢?针对这些改进，是为了解决什么的问题呢？请大家自行探索RSVD，消除用户和物品打分偏差等。

2. 矩阵分解的优缺点分析

   * 优点：
     * 泛化能力强： 一定程度上解决了稀疏问题
     * 空间复杂度低： 由于用户和物品都用隐向量的形式存放， 少了用户和物品相似度矩阵， 空间复杂度由$n^2$降到了$(n+m)*f$
     * 更好的扩展性和灵活性：矩阵分解的最终产物是用户和物品隐向量， 这个深度学习的embedding思想不谋而合， 因此矩阵分解的结果非常便于与其他特征进行组合和拼接， 并可以与深度学习无缝结合。

   + 缺点：
     + 矩阵分解算法依然是只用到了评分矩阵， 没有考虑到用户特征， 物品特征和上下文特征， 这使得矩阵分解丧失了利用很多有效信息的机会。
     + 同时在缺乏用户历史行为的时候， 无法进行有效的推荐。  
     + 为了解决这个问题， **逻辑回归模型及后续的因子分解机模型**， 凭借其天然的融合不同特征的能力， 逐渐在推荐系统领域得到了更广泛的应用。 

### 参考资料

* 王喆 - 《深度学习推荐系统》
* 项亮 - 《推荐系统实战》
* [奇异值分解(SVD)的原理详解及推导](https://blog.csdn.net/wuzhongqiang/article/details/108168238)
* [Matrix factorization techniques for recommender systems论文](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5197422&tag=1)
* [隐语义模型(LFM)和矩阵分解(MF)](https://blog.csdn.net/wuzhongqiang/article/details/108173885)



